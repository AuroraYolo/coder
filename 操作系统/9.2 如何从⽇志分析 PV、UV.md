## 别着急开始

```
当我们要分析⽇志的时候，先⽤ ls -lh 命令查看⽇志⽂件的⼤⼩，如果⽇志⽂件⼤⼩⾮常⼤，最好不要在
线上环境做。
⽐如我下⾯这个⽇志就 6.5M，不算⼤，在线上环境分析问题不⼤。
如果⽇志⽂件数据量太⼤，你直接⼀个 cat 命令⼀执⾏，是会影响线上环境，加重服务器的负载，严重
的话，可能导致服务器⽆响应。
当发现⽇志很⼤的时候，我们可以使⽤ scp 命令将⽂件传输到闲置的服务器再分析，scp 命令使⽤⽅式
```

## 慎⽤ cat
```
⼤家都知道 cat 命令是⽤来查看⽂件内容的，但是⽇志⽂件数据量有多少，它就读多少，很显然不适⽤
⼤⽂件。
对于⼤⽂件，我们应该养成好习惯，⽤ less 命令去读⽂件⾥的内容，因为 less 并不会加载整个⽂件，⽽
是按需加载，先是输出⼀⼩⻚的内容，当你要往下看的时候，才会继续加载。
可以发现，nginx 的 access.log ⽇志每⼀⾏是⼀次⽤户访问的记录，从左到右分别包含如下信息：
客户端的 IP 地址；
访问时间；
HTTP 请求的⽅法、路径、协议版本、协议版本、返回的状态码；
User Agent，⼀般是客户端使⽤的操作系统以及版本、浏览器及版本等；
不过，有时候我们想看⽇志最新部分的内容，可以使⽤ tail 命令，⽐如当你想查看倒数 5 ⾏的内容，你
可以使⽤这样的命令：
如果你想实时看⽇志打印的内容，你可以使⽤ tail -f 命令，这样你看⽇志的时候，就会是阻塞状态，有新
⽇志输出的时候，就会实时显示出来。
```

## PV 分析
````
PV 的全称叫 Page View，⽤户访问⼀个⻚⾯就是⼀次 PV，⽐如⼤多数博客平台，点击⼀次⻚⾯，阅读量
就加 1，所以说 PV 的数量并不代表真实的⽤户数量，只是个点击量。
对于 nginx 的 acess.log ⽇志⽂件来说，分析 PV 还是⽐较容易的，既然⽇志⾥的内容是访问记录，那有
多少条⽇志记录就有多少 PV。
我们直接使⽤ wc -l 命令，就可以查看整体的 PV 了，如下图⼀共有 49903 条 PV。
````

## PV 分组

```
nginx 的 acess.log ⽇志⽂件有访问时间的信息，因此我们可以根据访问时间进⾏分组，⽐如按天分组，
查看每天的总 PV，这样可以得到更加直观的数据。
要按时间分组，⾸先我们先「访问时间」过滤出来，这⾥可以使⽤ awk 命令来处理，awk 是⼀个处理⽂本
的利器。
awk 命令默认是以「空格」为分隔符，由于访问时间在⽇志⾥的第 4 列，因此可以使⽤ awk '{print $4}'
access.log 命令把访问时间的信息过滤出来，结果如下：
上⾯的信息还包含了时分秒，如果只想显示年⽉⽇的信息，可以使⽤ awk 的 substr 函数，从第 2 个字
符开始，截取 11 个字符。
接着，我们可以使⽤ sort 对⽇期进⾏排序，然后使⽤ uniq -c 进⾏统计，于是按天分组的 PV 就出来
了。
可以看到，每天的 PV 量⼤概在 2000-2800：
注意，使⽤ uniq -c 命令前，先要进⾏ sort 排序，因为 uniq 去重的原理是⽐较相邻的⾏，然后除去第
⼆⾏和该⾏的后续副本，因此在使⽤ uniq 命令之前，请使⽤ sort 命令使所有重复⾏相邻
```

## UV 分析
```
UV 的全称是 Uniq Visitor，它代表访问⼈数，⽐如公众号的阅读量就是以 UV 统计的，不管单个⽤户点击
了多少次，最终只算 1 次阅读量。
access.log ⽇志⾥虽然没有⽤户的身份信息，但是我们可以⽤「客户端 IP 地址」来近似统计 UV。
该命令的输出结果是 2589，也就说明 UV 的量为 2589。上图中，从左到右的命令意思如下：
awk '{print $1}' access.log ，取⽇志的第 1 列内容，客户端的 IP 地址正是第 1 列；
sort ，对信息排序；
uniq ，去除重复的记录；
wc -l ，查看记录条数；
```
## UV 分组

```
假设我们按天来分组分析每天的 UV 数量，这种情况就稍微⽐较复杂，需要⽐较多的命令来实现。
既然要按天统计 UV，那就得把「⽇期 + IP地址」过滤出来，并去重，命令如下：
具体分析如下：
第⼀次 ack 是将第 4 列的⽇期和第 1 列的客户端 IP 地址过滤出来，并⽤空格拼接起来；
然后 sort 对第⼀次 ack 输出的内容进⾏排序；
接着⽤ uniq 去除重复的记录，也就说⽇期 +IP 相同的⾏就只保留⼀个；
上⾯只是把 UV 的数据列了出来，但是并没有统计出次数。
如果需要对当天的 UV 统计，在上⾯的命令再拼接 awk '{uv[$1]++;next}END{for (ip in uv) print ip, uv[ip]}'
命令就可以了，结果如下图：
awk 本身是「逐⾏」进⾏处理的，当执⾏完⼀⾏后，我们可以⽤ next 关键字来告诉 awk 跳转到下⼀
⾏，把下⼀⾏作为输⼊。
对每⼀⾏输⼊，awk 会根据第 1 列的字符串（也就是⽇期）进⾏累加，这样相同⽇期的 ip 地址，就会累加
起来，作为当天的 uv 数量。
之后的 END 关键字代表⼀个触发器，就是当前⾯的输⼊全部完成后，才会执⾏ END {} 中的语句，END
的语句是通过 foreach 遍历 uv 中所有的 key，打印出按天分组的 uv 数量。
```

## 终端分析

```
nginx 的 access.log ⽇志最末尾关于 User Agent 的信息，主要是客户端访问服务器使⽤的⼯具，可能是⼿
机、浏览器等。
因此，我们可以利⽤这⼀信息来分析有哪些终端访问了服务器。
User Agent 的信息在⽇志⾥的第 12 列，因此我们先使⽤ awk 过滤出第 12 列的内容后，进⾏ sort 排
序，再⽤ uniq -c 去重并统计，最后再使⽤ sort -rn （r 表示逆向排序， n 表示按数值排序） 对统计的
结果排序，结果如下图：
分析 TOP3 的请求
access.log ⽇志中，第 7 列是客户端请求的路径，先使⽤ awk 过滤出第 7 列的内容后，进⾏ sort 排
序，再⽤ uniq -c 去重并统计，然后再使⽤ sort -rn 对统计的结果排序，最后使⽤ head -n 3 分析
TOP3 的请求，结果如下图：
```
