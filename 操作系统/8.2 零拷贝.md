## 什么是DMA技术?

```
在进⾏ I/O 设备和内存的数据传输的时候，数据搬运的⼯作全部交给
DMA 控制器，⽽ CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务

⽤户进程调⽤ read ⽅法，向操作系统发出 I/O 请求，请求读取数据到⾃⼰的内存缓冲区中，进程进
⼊阻塞状态；
操作系统收到请求后，进⼀步将 I/O 请求发送 DMA，然后让 CPU 执⾏其他任务；
DMA 进⼀步将 I/O 请求发送给磁盘；
磁盘收到 DMA 的 I/O 请求，把数据从磁盘读取到磁盘控制器的缓冲区中，当磁盘控制器的缓冲区被
读满后，向 DMA 发起中断信号，告知⾃⼰缓冲区已满；
DMA 收到磁盘的信号，将磁盘控制器缓冲区中的数据拷⻉到内核缓冲区中，此时不占⽤ CPU，CPU
可以执⾏其他任务； 当 DMA 读取了⾜够多的数据，就会发送中断信号给 CPU；
CPU 收到 DMA 的信号，知道数据已经准备好，于是将数据从内核拷⻉到⽤户空间，系统调⽤返回；
```

## 传统的⽂件传输有多糟糕？
```
期间共发⽣了 4 次⽤户态与内核态的上下⽂切换，因为发⽣了两次系统调⽤，⼀次是 read() ，⼀
次是 write() ，每次系统调⽤都得先从⽤户态切换到内核态，等内核完成任务后，再从内核态切换回⽤户
态。
上下⽂切换到成本并不⼩，⼀次切换需要耗时⼏⼗纳秒到⼏微秒，虽然时间看上去很短，但是在⾼并发的
场景下，这类时间容易被累积和放⼤，从⽽影响系统的性能。
其次，还发⽣了 4 次数据拷⻉，其中两次是 DMA 的拷⻉，另外两次则是通过 CPU 拷⻉的，下⾯说⼀下这
个过程：
第⼀次拷⻉，把磁盘上的数据拷⻉到操作系统内核的缓冲区⾥，这个拷⻉的过程是通过 DMA 搬运
的。
第⼆次拷⻉，把内核缓冲区的数据拷⻉到⽤户的缓冲区⾥，于是我们应⽤程序就可以使⽤这部分数据
了，这个拷⻉到过程是由 CPU 完成的。
第三次拷⻉，把刚才拷⻉到⽤户的缓冲区⾥的数据，再拷⻉到内核的 socket 的缓冲区⾥，这个过程
依然还是由 CPU 搬运的。
第四次拷⻉，把内核的 socket 缓冲区⾥的数据，拷⻉到⽹卡的缓冲区⾥，这个过程⼜是由 DMA 搬运
的。
我们回过头看这个⽂件传输的过程，我们只是搬运⼀份数据，结果却搬运了 4 次，过多的数据拷⻉⽆疑会
消耗 CPU 资源，⼤⼤降低了系统性能。
这种简单⼜传统的⽂件传输⽅式，存在冗余的上⽂切换和数据拷⻉，在⾼并发系统⾥是⾮常糟糕的，多了
很多不必要的开销，会严重影响系统性能。
所以，要想提⾼⽂件传输的性能，就需要减少「⽤户态与内核态的上下⽂切换」和「内存拷⻉」的次数。
```
## 如何优化⽂件传输的性能？

```
先来看看，如何减少「⽤户态与内核态的上下⽂切换」的次数呢？
读取磁盘数据的时候，之所以要发⽣上下⽂切换，这是因为⽤户空间没有权限操作磁盘或⽹卡，内核的权
限最⾼，这些操作设备的过程都需要交由操作系统内核来完成，所以⼀般要通过内核去完成某些任务的时
候，就需要使⽤操作系统提供的系统调⽤函数。
⽽⼀次系统调⽤必然会发⽣ 2 次上下⽂切换：⾸先从⽤户态切换到内核态，当内核执⾏完任务后，再切换
回⽤户态交由进程代码执⾏。
所以，要想减少上下⽂切换到次数，就要减少系统调⽤的次数。
再来看看，如何减少「数据拷⻉」的次数？
在前⾯我们知道了，传统的⽂件传输⽅式会历经 4 次数据拷⻉，⽽且这⾥⾯，「从内核的读缓冲区拷⻉到
⽤户的缓冲区⾥，再从⽤户的缓冲区⾥拷⻉到 socket 的缓冲区⾥」，这个过程是没有必要的。
因为⽂件传输的应⽤场景中，在⽤户空间我们并不会对数据「再加⼯」，所以数据实际上可以不⽤搬运到
⽤户空间，因此⽤户的缓冲区是没有必要存在的。
```

## 如何实现零拷贝
```
mmap + write
sendfile

mmap + write

在前⾯我们知道， read() 系统调⽤的过程中会把内核缓冲区的数据拷⻉到⽤户的缓冲区⾥，于是为了减少
这⼀步开销，我们可以⽤ mmap() 替换 read() 系统调⽤函数。
buf = mmap(file, len);
write(sockfd, buf, len);
mmap() 系统调⽤函数会直接把内核缓冲区⾥的数据「映射」到⽤户空间，这样，操作系统内核与⽤户空
间就不需要再进⾏任何的数据拷⻉操作。
具体过程如下：
应⽤进程调⽤了 mmap() 后，DMA 会把磁盘的数据拷⻉到内核的缓冲区⾥。接着，应⽤进程跟操作
系统内核「共享」这个缓冲区；
应⽤进程再调⽤ write() ，操作系统直接将内核缓冲区的数据拷⻉到 socket 缓冲区中，这⼀切都发⽣
在内核态，由 CPU 来搬运数据；
最后，把内核的 socket 缓冲区⾥的数据，拷⻉到⽹卡的缓冲区⾥，这个过程是由 DMA 搬运的。
我们可以得知，通过使⽤ mmap() 来代替 read() ， 可以减少⼀次数据拷⻉的过程。
但这还不是最理想的零拷⻉，因为仍然需要通过 CPU 把内核缓冲区的数据拷⻉到 socket 缓冲区⾥，⽽且
仍然需要 4 次上下⽂切换，因为系统调⽤还是 2 次。

sendfile

在 Linux 内核版本 2.1 中，提供了⼀个专⻔发送⽂件的系统调⽤函数 sendfile() ，函数形式如下：
#include <sys/socket.h>
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
它的前两个参数分别是⽬的端和源端的⽂件描述符，后⾯两个参数是源端的偏移量和复制数据的⻓度，返
回值是实际复制数据的⻓度。
⾸先，它可以替代前⾯的 read() 和 write() 这两个系统调⽤，这样就可以减少⼀次系统调⽤，也就减少
了 2 次上下⽂切换的开销。
其次，该系统调⽤，可以直接把内核缓冲区⾥的数据拷⻉到 socket 缓冲区⾥，不再拷⻉到⽤户态，这样就
只有 2 次上下⽂切换，和 3 次数据拷⻉

但是这还不是真正的零拷⻉技术，如果⽹卡⽀持 SG-DMA（The Scatter-Gather Direct Memory Access）
技术（和普通的 DMA 有所不同），我们可以进⼀步减少通过 CPU 把内核缓冲区⾥的数据拷⻉到 socket
缓冲区的过程。
你可以在你的 Linux 系统通过下⾯这个命令，查看⽹卡是否⽀持 scatter-gather 特性：
$ ethtool -k eth0 | grep scatter-gather
scatter-gather: on
于是，从 Linux 内核 2.4 版本开始起，对于⽀持⽹卡⽀持 SG-DMA 技术的情况下， sendfile() 系统调
⽤的过程发⽣了点变化，具体过程如下：
第⼀步，通过 DMA 将磁盘上的数据拷⻉到内核缓冲区⾥；
第⼆步，缓冲区描述符和数据⻓度传到 socket 缓冲区，这样⽹卡的 SG-DMA 控制器就可以直接将内
核缓存中的数据拷⻉到⽹卡的缓冲区⾥，此过程不需要将数据从操作系统内核缓冲区拷⻉到 socket
缓冲区中，这样就减少了⼀次数据拷⻉；
所以，这个过程之中，只进⾏了 2 次数据拷⻉

所谓的零拷⻉（Zero-copy）技术，因为我们没有在内存层⾯去拷⻉数据，也就是说全程没有通过
CPU 来搬运数据，所有的数据都是通过 DMA 来进⾏传输的。。
零拷⻉技术的⽂件传输⽅式相⽐传统⽂件传输的⽅式，减少了 2 次上下⽂切换和数据拷⻉次数，只需要 2
次上下⽂切换和数据拷⻉次数，就可以完成⽂件的传输，⽽且 2 次的数据拷⻉过程，都不需要通过 CPU，
2 次都是由 DMA 来搬运。
所以，总体来看，零拷⻉技术可以把⽂件传输的性能提⾼⾄少⼀倍以上。
```
## PageCache 有什么作⽤？
````
核缓冲区」实际上是磁盘⾼速缓存（PageCache）。
由于零拷⻉使⽤了 PageCache 技术，可以使得零拷⻉进⼀步提升了性能，我们接下来看看 PageCache
是如何做到这⼀点的。
读写磁盘相⽐读写内存的速度慢太多了，所以我们应该想办法把「读写磁盘」替换成「读写内存」。于
是，我们会通过 DMA 把磁盘⾥的数据搬运到内存⾥，这样就可以⽤读内存替换读磁盘。
但是，内存空间远⽐磁盘要⼩，内存注定只能拷⻉磁盘⾥的⼀⼩部分数据。
那问题来了，选择哪些磁盘数据拷⻉到内存呢？
我们都知道程序运⾏的时候，具有「局部性」，所以通常，刚被访问的数据在短时间内再次被访问的概率
很⾼，于是我们可以⽤ PageCache 来缓存最近被访问的数据，当空间不⾜时淘汰最久未被访问的缓存。
所以，读磁盘数据的时候，优先在 PageCache 找，如果数据存在则可以直接返回；如果没有，则从磁盘
中读取，然后缓存 PageCache 中。
还有⼀点，读取磁盘数据的时候，需要找到数据所在的位置，但是对于机械磁盘来说，就是通过磁头旋转
到数据所在的扇区，再开始「顺序」读取数据，但是旋转磁头这个物理动作是⾮常耗时的，为了降低它的
影响，PageCache 使⽤了「预读功能」。
⽐如，假设 read ⽅法每次只会读 32 KB 的字节，虽然 read 刚开始只会读 0 ～ 32 KB 的字节，但内核会
把其后⾯的 32～64 KB 也读取到 PageCache，这样后⾯读取 32～64 KB 的成本就很低，如果在 32～64
KB 淘汰出 PageCache 前，进程读取到它了，收益就⾮常⼤。
所以，PageCache 的优点主要是两个：
缓存最近被访问的数据；
预读功能；
这两个做法，将⼤⼤提⾼读写磁盘的性能。
但是，在传输⼤⽂件（GB 级别的⽂件）的时候，PageCache 会不起作⽤，那就⽩⽩浪费 DMA 多做的⼀
次数据拷⻉，造成性能的降低，即使使⽤了 PageCache 的零拷⻉也会损失性能
这是因为如果你有很多 GB 级别⽂件需要传输，每当⽤户访问这些⼤⽂件的时候，内核就会把它们载⼊
PageCache 中，于是 PageCache 空间很快被这些⼤⽂件占满。
另外，由于⽂件太⼤，可能某些部分的⽂件数据被再次访问的概率⽐较低，这样就会带来 2 个问题：
PageCache 由于⻓时间被⼤⽂件占据，其他「热点」的⼩⽂件可能就⽆法充分使⽤到 PageCache，
于是这样磁盘读写的性能就会下降了；
PageCache 中的⼤⽂件数据，由于没有享受到缓存带来的好处，但却耗费 DMA 多拷⻉到
PageCache ⼀次；
所以，针对⼤⽂件的传输，不应该使⽤ PageCache，也就是说不应该使⽤零拷⻉技术，因为可能由于
PageCache 被⼤⽂件占据，⽽导致「热点」⼩⽂件⽆法利⽤到 PageCache，这样在⾼并发的环境下，会
带来严重的性能问题。
````

## ⼤⽂件传输⽤什么⽅式实现？
````
在⾼并发的场景下，针对⼤⽂件的传输的⽅式，应该使⽤「异步 I/O + 直接 I/O」来替代零拷⻉技
术。
直接 I/O 应⽤场景常⻅的两种：
应⽤程序已经实现了磁盘数据的缓存，那么可以不需要 PageCache 再次缓存，减少额外的性能损
耗。在 MySQL 数据库中，可以通过参数设置开启直接 I/O，默认是不开启；
传输⼤⽂件的时候，由于⼤⽂件难以命中 PageCache 缓存，⽽且会占满 PageCache 导致「热点」
⽂件⽆法充分利⽤缓存，从⽽增⼤了性能开销，因此，这时应该使⽤直接 I/O。
另外，由于直接 I/O 绕过了 PageCache，就⽆法享受内核的这两点的优化：
内核的 I/O 调度算法会缓存尽可能多的 I/O 请求在 PageCache 中，最后「合并」成⼀个更⼤的 I/O
请求再发给磁盘，这样做是为了减少磁盘的寻址操作；
内核也会「预读」后续的 I/O 请求放在 PageCache 中，⼀样是为了减少对磁盘的操作；
于是，传输⼤⽂件的时候，使⽤「异步 I/O + 直接 I/O」了，就可以⽆阻塞地读取⽂件了。
所以，传输⽂件的时候，我们要根据⽂件的⼤⼩来使⽤不同的⽅式：
传输⼤⽂件的时候，使⽤「异步 I/O + 直接 I/O」；
传输⼩⽂件的时候，则使⽤「零拷⻉技术」；
````

## 总结
```
早期 I/O 操作，内存与磁盘的数据传输的⼯作都是由 CPU 完成的，⽽此时 CPU 不能执⾏其他任务，会特
别浪费 CPU 资源。
于是，为了解决这⼀问题，DMA 技术就出现了，每个 I/O 设备都有⾃⼰的 DMA 控制器，通过这个 DMA
控制器，CPU 只需要告诉 DMA 控制器，我们要传输什么数据，从哪⾥来，到哪⾥去，就可以放⼼离开
了。后续的实际数据传输⼯作，都会由 DMA 控制器来完成，CPU 不需要参与数据传输的⼯作。
传统 IO 的⼯作⽅式，从硬盘读取数据，然后再通过⽹卡向外发送，我们需要进⾏ 4 上下⽂切换，和 4 次
数据拷⻉，其中 2 次数据拷⻉发⽣在内存⾥的缓冲区和对应的硬件设备之间，这个是由 DMA 完成，另外
2 次则发⽣在内核态和⽤户态之间，这个数据搬移⼯作是由 CPU 完成的。
为了提⾼⽂件传输的性能，于是就出现了零拷⻉技术，它通过⼀次系统调⽤（ sendfile ⽅法）合并了磁盘
读取与⽹络发送两个操作，降低了上下⽂切换次数。另外，拷⻉数据都是发⽣在内核中的，天然就降低了
数据拷⻉的次数。
Kafka 和 Nginx 都有实现零拷⻉技术，这将⼤⼤提⾼⽂件传输的性能。
零拷⻉技术是基于 PageCache 的，PageCache 会缓存最近访问的数据，提升了访问缓存数据的性能，同
时，为了解决机械硬盘寻址慢的问题，它还协助 I/O 调度算法实现了 IO 合并与预读，这也是顺序读⽐随机
读性能好的原因。这些优势，进⼀步提升了零拷⻉的性能。
需要注意的是，零拷⻉技术是不允许进程对⽂件内容作进⼀步的加⼯的，⽐如压缩数据再发送。
另外，当传输⼤⽂件时，不能使⽤零拷⻉，因为可能由于 PageCache 被⼤⽂件占据，⽽导致「热点」⼩
⽂件⽆法利⽤到 PageCache，并且⼤⽂件的缓存命中率不⾼，这时就需要使⽤「异步 IO + 直接 IO 」的⽅
式。
在 Nginx ⾥，可以通过配置，设定⼀个⽂件⼤⼩阈值，针对⼤⽂件使⽤异步 IO 和直接 IO，⽽对⼩⽂件使
⽤零拷⻉。
```
